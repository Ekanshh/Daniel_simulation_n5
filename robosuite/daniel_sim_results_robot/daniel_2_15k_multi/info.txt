    control_param = dict(type='IMPEDANCE_POSE_Partial', input_max=1, input_min=-1,
                         output_max=[0.05, 0.05, 0.05, 0.5, 0.5, 0.5],
                         output_min=[-0.05, -0.05, -0.05, -0.5, -0.5, -0.5], kp=700, damping_ratio=np.sqrt(2),
                         impedance_mode='fixed', kp_limits=[0, 100000], damping_ratio_limits=[0, 10],
                         position_limits=None, orientation_limits=None, uncouple_pos_ori=True, control_delta=True,
                         interpolation=None, ramp_ratio=0.2, control_dim=26, plotter=False, ori_method='rotation',
                         show_params=False)

    env_id = 'PegInHoleSmall'

    env_options = dict(robots="UR5e", use_camera_obs=False, has_offscreen_renderer=False, has_renderer=False,
                       reward_shaping=True, ignore_done=False, horizon=500, control_freq=20,
                       controller_configs=control_param, r_reach_value=0.2, tanh_value=20.0, error_type='ring',
                       control_spec=26, dist_error=0.0008)
    n_steps = 50
    seed_val = 4
    num_proc = 10
    #
    env = SubprocVecEnv([make_robosuite_env(env_id, env_options, i, seed_val) for i in range(num_proc)])
    # eval_callback = CheckpointCallback(save_freq=1, save_path='./checkpoints/',
    #                             name_prefix=log_dir, verbose=2)
    # eval_callback = EvalCallback(env,
    #                              best_model_save_path=log_dir,
    #                              log_path=log_dir,
    #                              eval_freq=2,
    #                              deterministic=False,
    #                              render=False)

    checkpoint_callback = CheckpointCallback(save_freq=2, save_path=log_dir,
                                             name_prefix='rl_model')
    # reward_callback = SaveOnBestTrainingRewardCallback(check_freq=3, log_dir=log_dir)
    # env = VecNormalize(env)
    policy_kwargs = dict(activation_fn=torch.nn.LeakyReLU, net_arch=[32, 32])
    model = PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs, n_steps=int(n_steps/num_proc),
                tensorboard_log="./learning_log/ppo_tensorboard/", seed=4)

    model.learn(total_timesteps=15_000, tb_log_name="learning", callback=checkpoint_callback, reset_num_timesteps=True)
    print("Done")
    model.save('Daniel_2_n_step50_no_seed_n_proc10')


